---
title: "Domača naloga 3"
author: "Neža Kržan, Tom Rupnik Medjedovič"
output:
  pdf_document:
    fig_caption: true
    number_sections: true
header-includes:
- \usepackage[slovene]{babel}
- \usepackage{float}
- \usepackage[T1]{fontenc}
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", fig.pos = "H", message = FALSE, warning = FALSE, results = F, fig.height =3, fig.width = 6.5)

# potrebne knjižnice
library(ggplot2)
library(car)
library(tidyr)
library(dplyr)
library(ADGofTest)
library(lmerTest)
library(knitr)
library(kableExtra)

# seme
set.seed(2024)
```

# Cilji naloge

<!-- Jst nevem če tole čist razumem... midva morva primerjat dve resampling methods? 
Delava linearno regresijo, kjer mava krseni dve predpostavki, torej iz tega dobiva original rezultate, pol bova pa delala bootstrap pa se eno?
Kaj pa potem spreminjava da vplivava na primernost in uspesnot metod? Mogoce velikost vzorca? Ce mas majhen vzorec pa velik bootrtrap vzorcev najbr ne bo ok al kaj?

Aa okej dojamem zdej, najina klasična metoda je linearna regresija, najina druga metoda je pa bootstrap?

In simulations, vary at least two factors that could influence the suitability or success of the methods. - 
Torej modva bova za to spreminjala linearnost pa heteroskedasticnost?
Sam, ne razumem cist dobr, al morva spreminjat tud neki kar bi vplival na bootstrap?

Tom16: ja jst sm tako razumel, da je najna osnovna metoda lin. reg. ampak ker zaradi krsenja predpostavk ni dobra metoda primerjava še z bootstrap metodo. 
Zdej se mi zdi da spreminjava dovolj faktorjev k imava še velikost vzorca...sicer pa ja heteroskedastičnost se lepo vidi, ta nelinernost je pa bolj boga haha-->

Želiva preučiti uporabo metode ponovnega vzorčenja, v primeru ko klasičnimi testom ne moremo popolno zaupati zaradi kršenja predpostavk. Generirala bova podatke, na katerih bova izračunala intervale zaupanja koeficientov linearne regresije. 

Za uporabno linearne regresije je potrebno izpolniti določene predpostavke, da imamo veljaven model. Te so:

- linearna odvisnost: _obstoj linearne povezanosti med napovednimi (pojasnjevalnimi) spremenljivkami in odzivno (ciljno) spremenljivko_,
- normalna porazdeljenost napak in neodvisnost napak,
- homoskedastičnost: _varianca napak mora biti konstantna_,
- brez multikolinearnosti: _neodvisne spremenljivke ne smejo biti preveč povezane med seboj_,
- zadostno število podatkov.

<!--Neza17: mislm da sm use nastela...
Tom16: ja se mi zdi da si kr use naštela-->

Podatke bova generirala tako, da bodo nekatere izmed teh predpostavk kršene(opisano v naslednjem poglavju) in zaradi tega bova s pomočjo testa `boxCox` izvedla primerno transformacijo odzivne spremenljivke (ta bo v vseh primerih `log` transformacija). Na koncu bova med seboj primerjala intervale zaupanja dobljene z linearno regresijo (`lm`) in metodo ponovnega vzorčenja(bootstrap in permutacijski test).<!--Tom16: bova tudi permutacijski test baredila?--> Primerjavo bova naredila tako na rezultatih pred in po transformaciji, vendar pa moramo paziti, saj rezultati pred in po transformaciji med seboj niso primerljivi. 

Pričakujeva, da bomo z metodo ponovnega vzorčenja dobili boljše rezultate.

# Generiranje podatkov

Podatke sva generirala tako, da je v linearnem regresiji kršena predpostavka linearne odvisnosti odzivne spremenljivke od napovednih in kršena homoskedastičnost (konstantna varianca napak). Enačba, ki sva jo uporabila za generiranje odzivne spremenljivke je enaka:

$$y_i = \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i}^\gamma + \epsilon_i$$
pri čemer:

* $\beta_0$-konstanta enaka $100$,
* $\beta_1$-koeficient spremenljivke $x_1$ enak $3$,
* $\beta_2$-koeficient spremenljivke $x_2$ enak $2$,
* $\gamma$-eksponent, ki ga bova spreminjala (določa nelinearno zvezo),
* $\epsilon$-napaka, ki generirana iz porazdelitve $N(0,x_1\cdot\alpha)$ ($\alpha$ določa povezanost s spremenljivko $x_1$),

torej

$$
y_i = 100 + 3 \cdot x_{1i} + 2 \cdot x_{2i}^\gamma + \epsilon_i.
$$

Kot sva že omenila bova spreminjala faktorja $\gamma$ in $\alpha$. S faktorjem $\gamma$ bomo kršili prespostavko o linearni zvezi, saj bo ta zavzel vrednosti $0.8$ in $1.4$. S faktorjem $\alpha$ pa bomo kršili predpostavko konstantne variance napak, saj se ta z večanjem vrednosti $x_1$ povečuje. Ta zavzame vrednosti $\alpha\in\{0.6, 1, 1.2\}$. 

Velikost vzorca pa enak  $n = 500$, saj je to dovolj velik vzorec za pravilno delovanje linearne regresije.

<!--Neza17: misls da morva kej napisat zakaj sva zbrala 100, 3, 2 pa potem za apha pa gamma?
Pa jst sm velikost vzorca povecala, ker mora bit velik
?
Tom16: ne men se zdi da to ne rabiva ravno utemeljevat...pac tko sva si izbrala...
-->

Pri generiranju posameznih vrednosti v enačbi linearne regresije($x_{1i}, x_{2i}$) sva se odločila za generiranje iz enakomerne porazdelitve, in sicer $x_{1i} \sim Unif(20, 60)$ ter $x_{2i} \sim Unif(2, 10)$, torej, da imamo v podatkih majhne vrednosti in nekoliko večje.

```{r generiranje podatkov}
set.seed(2024)
n = 500 # velikost vzorca

generiranje_podatkov <- function(beta0, beta1, beta2, alpha, gamma, n) {
  # generiranje vrednosti za x1 in x2
  x1 <- runif(n, 20, 60)
  x2 <- runif(n, 2, 10)
  
  # generiranje napake
  epsilon <- rnorm(n, 0,x1*alpha) # dodava heteroskedasticnost
  
  # izračun napovedne spremenljivke za model (parametri = vhodni argumenti) 
  y <- beta0 + beta1 * x1 + beta2 * x2^gamma + epsilon # z ^gamma dodava nelinearni del
  
  # podatke spravimo v podatkovni okvir in vrnemo kot rezultat funkcije
  data.frame(x1 = x1, x2 = x2, y = y) 
}

# izracun vrednosti vzorca
vzorec06_08 = generiranje_podatkov(100, 3, 2, 0.6, 0.8, n)
vzorec06_14 = generiranje_podatkov(100, 3, 2, 0.6, 1.4, n)
vzorec1_08 = generiranje_podatkov(100, 3, 2, 1, 0.8, n)
vzorec1_14 = generiranje_podatkov(100, 3, 2, 1, 1.4, n)
vzorec12_08 = generiranje_podatkov(100, 3, 2, 1.2, 0.8, n)
vzorec12_14 = generiranje_podatkov(100, 3, 2, 1.2, 1.4, n)


# modeli (brez transformacije)
model06_08 = lm(y ~ x1 + x2, vzorec06_08)
model06_14 = lm(y ~ x1 + x2, vzorec06_14)
model1_08 = lm(y ~ x1 + x2, vzorec1_08)
model1_14 = lm(y ~ x1 + x2, vzorec1_14)
model12_08 = lm(y ~ x1 + x2, vzorec12_08)
model12_14 = lm(y ~ x1 + x2, vzorec12_14)

# modeli (s transformacijo)
model06_08_log = lm(log(y) ~ x1 + x2, vzorec06_08)
model06_14_log = lm(log(y) ~ x1 + x2, vzorec06_14)
```

Narišimo grafe ostankov za nekaj kombinacij faktorjev, da se prepričamo o kršenju predpostavk, velikost vzorca je v *vseh* primerih nastavljena na $500$.

```{r "graf ostankov 0.6 in 0.8", fig.dim=c(5,3), fig.cap="Grafi ostankov pri paramatrih alpha=0,6 in gamma=0,8."}
par(mfrow=c(1,2))
plot(model06_08, which = c(1,3))
```

```{r "graf ostankov 1.2 in 1.4", fig.dim=c(5,3), fig.cap="Grafi ostankov pri paramatrih alpha=1,2 in gamma=1,4."}
par(mfrow=c(1,2))
plot(model12_14, which = c(1,3))
```
<!-- sori ampak jst tega ne vidim čist tko kt nsi ti napisu hah, sploh za levi graf, na katerih robovih točno? Jst bi rajs kej tko napisala za levi graf da pač je vidno da se varibilnost med podatki več ko gremo čez graf od leve proti desni v obeh primerih, pa da je za manjši gamma to večanje variabilnosti pač večje oziroma, da je bolj opazno no?
Tom16: tudi jst ne opazim vec tega na levem grafu haha ne k sva povecala vzorec se ne vidi vec tega k sm pisal za levi graf...bom zdej popravil neki na to foro k si napisala-->
V obeh primerih lahko na desnem grafu opazimo, da se z večanjem vrednosti povečuje tudi variabilnost napak (naraščajoč trend). To lahko opazimo, tudi iz levega grafa, saj se od leve proti desni s povečevanjem vrednosti, povečuje tudi variabilnost ostankov. Prav tako je vidna razlika, ko povečamo vrednost parametra $\alpha$, saj se vrednosti ostankov povečajo (variabilnost se poveča).  

Kot sva že napisala zgoraj, bova zaradi kršenja dveh predpostavk(linerana odvisnost in homoskedastičnost) podatke ustrezno transformirala. Za tako kršene predpostavke ponavadi uporabljamo logaritemske transformacije podatkov.\
Za izbiro primerne transformacije si bova pomagala s `boxCox` testom - za vsako kombinacijo parametrov preverimo ali se vrednost $\lambda=0$ (`log` transformacija) nahaja znotraj 95% intervala optimalnega parametra $\lambda$, ki ga vrne funkija `powerTransform`. V spodnji tabeli lahko vidimo, da je $\lambda=0$ res vsebovana v vseh 95% intervalih zaupanja, razen v zadnjem primeru, torej je primerna transformacija podatkov logaritemska. Pri zadnjem primeru, pa je spodnja meja 95% intervala zaupanja tako blizu vrendosti $0$, da prav tako lahko uporabimo logaritemsko transformacijo, ker bomo dobili boljše rezultate in bodo predopstavke bolje izpolnjene.

```{r lambda vrednsoti, results=T, }
spodnja = c(summary(powerTransform(model06_08))$result[3],
            summary(powerTransform(model06_14))$result[3],
            summary(powerTransform(model1_08))$result[3],
            summary(powerTransform(model1_14))$result[3],
            summary(powerTransform(model12_08))$result[3],
            summary(powerTransform(model12_14))$result[3])

zgornja = c(summary(powerTransform(model06_08))$result[4],
            summary(powerTransform(model06_14))$result[4],
            summary(powerTransform(model1_08))$result[4],
            summary(powerTransform(model1_14))$result[4],
            summary(powerTransform(model12_08))$result[4],
            summary(powerTransform(model12_14))$result[4])

opt_lambda = data.frame("alpha" = c(0.6, 0.6, 1, 1, 1.2, 1.2),
                        "gamma" = c(rep(c(0.8, 1.4), 3)), 
                        "spodnji" = round(spodnja, 3),
                        "zgornji" = round(zgornja,3))

kable(opt_lambda, align = "c", col.names = c("alpha", "gamma", "spodnja meja IZ", "zgornja meja IZ")) %>%
  kable_styling(position = "center")
```

Če si sedaj ponovno pogledamo grafe ostankov transformiranih podatkov z istimi kombinacijami faktorjev kot na zgornjih grafih ostankov, vidimo, da so ostanki na grafih razpršeni naključno, torej predpostavki(linearna odvisnost in homoskedastičnost) nista kršeni.

```{r "graf ostankov 0.6 in 0.8 + trans", fig.dim=c(5,3), fig.cap="Grafi ostankov transformiranih podatkov pri paramatrih alpha=0,6 in gamma=0,8."}
# modeli (brez transformacije)
model06_08_log = lm(log(y) ~ x1 + x2, vzorec06_08)
model06_14_log = lm(log(y) ~ x1 + x2, vzorec06_14)

par(mfrow=c(1,2))
plot(model06_08_log, which = c(1,3))
```

```{r "graf ostankov 0.6 in 1.4 + trans", fig.dim=c(5,3), fig.cap="Grafi ostankov transformiranih podatkov pri paramatrih alpha=0,6 in gamma=1,4."}
par(mfrow=c(1,2))
plot(model06_14_log, which = c(1,3))
```

# Klasični test in metoda ponovnega vzorčenja

Pri ponovnem vzorčenju bova uporabila metodo bootstrap, pri kateri bo število bootstrap vzorcev enako $m=1000$. 

Za vsako kombinacijo faktorjev( _alpha_, _gamma_ in _velikost vzorca_) sva generirala podatke, na katerih sva za vsako kombinacijo faktorjev torej izvedla linearno regresijo in poračunala intervale zaupanja za vse tri koeficiente(`Intercept`, `x1`, `x2`). \
Enak postopek sva ponovila z metodo ponovnega vzorčenja bootstrap - naključno sva iz generiranih podatkov za vse kombinacije faktorjev izbrala podatke, na katerih sva potem izvedla linearno regresijo in izračunala intervale zaupanja za vse tri koeficiente (`Intercept`, `x1`, `x2`). Tak postopek ponovnega vzorčenja sva ponovila $m = 1000$, zato smo torej imeli $1000$ bootstrap vzorcev.

## Analiza rezultatov podatkov brez transformacije

Jasno nam torej je, da se na rezultate, pridobljene s podatki pred transformacijo ne moremo ravno zanesti, saj kršenje predpostavk pri linearni regresiji močno vpliva na intervale zaupanja(tudi na ocene koeficientov). Pri majhnem vzorcu($n = 20$) seveda pričakujemo najširše intervale zaupanja, ki pa se potem z večanjem vzorca ožajo. Verjetno bodo intervali zaupanja pri obeh metodah približno enako široki. 

<!-- misls da bi mogla v intervale dodat se ocene za koeficiente? pac a nismo to delal pr linearnih modelih da smo gledal kam pade v IZ ocena koeficienta? pa bi pr bootstrapu dala pac povprecje vseh ocen v vseh vzorcih?-->

```{r}
IZ_org = readRDS("intervali.zaupanja.org.RDS") %>% 
  select(-c("int.coef", "x1.coef", "x2.coef"))
IZ_bootstrap = readRDS("intervali.zaupanja.RDS")

IZ_skupaj = rbind(IZ_org, IZ_bootstrap)
IZ_skupaj = cbind(IZ_skupaj, rep(c("klasična analiza(linearna regresija)", "bootstrap"), each=nrow(IZ_org)))
colnames(IZ_skupaj)[10] = "metoda"

resLong = pivot_longer(IZ_skupaj, cols =matches("^(int|x1|x2)\\."),
                       values_to = "value",
                       names_to = c("parameter", "tip"),
                       names_pattern = "^(int|x1|x2)\\.(lower|upper)") 
```

```{r "risanje IZ pred transformacijo int", fig.cap="Grafi intervalov zaupanja za prosti koeficient(Intercept)."}
# pod_int = resLong %>%
#   filter(parameter == "int")

pod_int = IZ_skupaj %>% select(c("alpha", "gamma", "velikost.vzorca", "int.lower", "int.upper", "metoda"))
pod_int$velikost.vzorca = factor(pod_int$velikost.vzorca)

custom_labeller <- labeller(
  alpha = function(x) paste("\u03b1 =", x),
  gamma = function(x) paste("\u03b3 =", x)
)

ggplot(pod_int, aes(x=velikost.vzorca, col=metoda, group=metoda)) +
  geom_errorbar(aes(ymin = int.lower, ymax = int.upper), width=0.5, position = position_dodge2(reverse = TRUE, 0.3)) + 
  facet_grid(alpha~gamma, scales="free", labeller = custom_labeller) +
  labs(x = "velikost vzorca (n)", y = " ") +
  theme(legend.position = "bottom") +
  theme_minimal()
```

```{r "risanje IZ pred transformacijo x1", fig.cap="Grafi intervalov zaupanja za koeficient pri x1(beta1)."}
pod_x1 = IZ_skupaj %>% select(c("alpha", "gamma", "velikost.vzorca", "x1.lower", "x1.upper", "metoda"))
pod_x1$velikost.vzorca = factor(pod_x1$velikost.vzorca)

ggplot(pod_x1, aes(x=velikost.vzorca, col=metoda, group=metoda)) +
  geom_errorbar(aes(ymin = x1.lower, ymax = x1.upper), width=0.5, position = position_dodge2(reverse = TRUE, 0.3)) + 
  facet_grid(alpha~gamma, scales="free", labeller = custom_labeller) +
  labs(x = "velikost vzorca (n)", y = " ") +
  theme(legend.position = "bottom") +
  theme_minimal()
```

```{r "risanje IZ pred transformacijo x2", fig.cap="Grafi intervalov zaupanja za koeficient pri x2(beta2)."}
pod_x2 = IZ_skupaj %>% select(c("alpha", "gamma", "velikost.vzorca", "x2.lower", "x2.upper", "metoda"))
pod_x2$velikost.vzorca = factor(pod_x1$velikost.vzorca)

ggplot(pod_x2, aes(x=velikost.vzorca, col=metoda, group=metoda)) +
  geom_errorbar(aes(ymin = x2.lower, ymax = x2.upper), width=0.5, position = position_dodge2(reverse = TRUE, 0.3)) + 
  facet_grid(alpha~gamma, scales="free", labeller = custom_labeller) +
  labs(x = "velikost vzorca (n)", y = " ") +
  theme(legend.position = "bottom") +
  theme_minimal()
```

## Analiza rezultatov podatkov z transformacijo

```{r}
IZ_org_transf = readRDS("intervali.zaupanja.org.transf.RDS") %>% 
  select(-c("int.coef", "x1.coef", "x2.coef"))
IZ_bootstrap_transf = readRDS("intervali.zaupanja.transf.RDS")

IZ_skupaj_transf = rbind(IZ_org_transf, IZ_bootstrap_transf)
IZ_skupaj_transf = cbind(IZ_skupaj_transf, rep(c("klasična analiza(linearna regresija)", "bootstrap"), each=nrow(IZ_org)))
colnames(IZ_skupaj_transf)[10] = "metoda"

resLong_transf = pivot_longer(IZ_skupaj_transf, cols =matches("^(int|x1|x2)\\."),
                       values_to = "value",
                       names_to = c("parameter", "tip"),
                       names_pattern = "^(int|x1|x2)\\.(lower|upper)") 
```

```{r "risanje IZ po transformacijo int", fig.cap="Grafi intervalov zaupanja za prosti koeficient(Intercept)."}
# pod_int = resLong %>%
#   filter(parameter == "int")

pod_int = IZ_skupaj_transf %>% select(c("alpha", "gamma", "velikost.vzorca", "int.lower", "int.upper", "metoda"))
pod_int$velikost.vzorca = factor(pod_int$velikost.vzorca)

custom_labeller <- labeller(
  alpha = function(x) paste("\u03b1 =", x),
  gamma = function(x) paste("\u03b3 =", x)
)

ggplot(pod_int, aes(x=velikost.vzorca, col=metoda, group=metoda)) +
  geom_errorbar(aes(ymin = int.lower, ymax = int.upper), width=0.5, position = position_dodge2(reverse = TRUE, 0.3)) + 
  facet_grid(alpha~gamma, scales="free", labeller = custom_labeller) +
  labs(x = "velikost vzorca (n)", y = " ") +
  theme(legend.position = "bottom") +
  theme_minimal()
```

```{r "risanje IZ po transformacijo x1", fig.cap="Grafi intervalov zaupanja za koeficient pri x1(beta1)."}
pod_x1 = IZ_skupaj_transf %>% select(c("alpha", "gamma", "velikost.vzorca", "x1.lower", "x1.upper", "metoda"))
pod_x1$velikost.vzorca = factor(pod_x1$velikost.vzorca)

ggplot(pod_x1, aes(x=velikost.vzorca, col=metoda, group=metoda)) +
  geom_errorbar(aes(ymin = x1.lower, ymax = x1.upper), width=0.5, position = position_dodge2(reverse = TRUE, 0.3)) + 
  facet_grid(alpha~gamma, scales="free", labeller = custom_labeller) +
  labs(x = "velikost vzorca (n)", y = " ") +
  theme(legend.position = "bottom") +
  theme_minimal()
```

```{r "risanje IZ po transformacijo x2", fig.cap="Grafi intervalov zaupanja za koeficient pri x2(beta2)."}
pod_x2 = IZ_skupaj_transf %>% select(c("alpha", "gamma", "velikost.vzorca", "x2.lower", "x2.upper", "metoda"))
pod_x2$velikost.vzorca = factor(pod_x1$velikost.vzorca)

ggplot(pod_x2, aes(x=velikost.vzorca, col=metoda, group=metoda)) +
  geom_errorbar(aes(ymin = x2.lower, ymax = x2.upper), width=0.5, position = position_dodge2(reverse = TRUE, 0.3)) + 
  facet_grid(alpha~gamma, scales="free", labeller = custom_labeller) +
  labs(x = "velikost vzorca (n)", y = " ") +
  theme(legend.position = "bottom") +
  theme_minimal()
```