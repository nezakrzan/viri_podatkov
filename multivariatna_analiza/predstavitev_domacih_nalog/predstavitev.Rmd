---
title: "Predstavitev domačih nalog"
author: "Neža Kržan, Tom Rupnik"
output:
  pdf_document:
    fig_caption: true
    number_sections: true
  # word_document: default
header-includes:
- \usepackage[slovene]{babel}
- \usepackage{float}
- \usepackage[T1]{fontenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.pos = "H",
	message = FALSE,
	warning = FALSE,
	results = F, fig.height = 4, fig.width = 5
)

library(kableExtra)
library(MASS)
library(mclust)
library(haven)
library(psych)
library(ggplot2)
library(reshape2)
library(dplyr)
library(kableExtra)
library(blockmodeling)

set.seed(2024)
```


# Metoda glavnih komponent

## Slide 1

Podatki testa osebnosti - spremenljivke energije in čustvene stabilnosti. Želiva pojasniti, s prvimi dvema nekoreliranima glavnima komponentama kar, največ variabilnosti originalnih spremenljivk in zmanjšati razsežnost podatkov tako, da se izgubi čim manj informacij.

```{r}
# vsi podatki
dataframe <- read_sav("dn3_big5.sav") 

# samo najni stolpci
stolpci = c("EXT1","EXT2","EXT3","EXT4","EXT5","EXT6","EXT7","EXT8","EXT9","EXT10",
            "EST1","EST2","EST3","EST4","EST5","EST6","EST7","EST8","EST9","EST10")

# najni podatki
podatki = dataframe[dataframe$country == "FR", stolpci]
n_prej = nrow(podatki) # stevilo vrstic pred odstranitvijo NA
podatki = na.omit(podatki) # odstranjene vrstice, ki vsebujejo NA
razlika = n_prej - nrow(podatki) # stevilo odtrajenih vrstic

negativne = c("EXT2", "EXT4", "EXT6", "EXT8", "EXT10",
              "EST1", "EST3", "EST5", "EST6", "EST7", "EST8", "EST9", "EST10")

# negativne = c("EXT2", "EXT4", "EXT6", "EXT8", "EXT10",
#               "EST2", "EST4")

for(el in negativne){
  podatki[[el]] = -1*podatki[[el]]
}
Rlik <- cor(podatki)
mgk <- princomp(x=podatki, cor=T, scores=T)
mgk <- princomp(x=podatki, cor=T, scores=T)
loadings.orig <- mgk$loadings[,1:2]
eig <- eigen(x=Rlik, symmetric=T, only.values=T)$values
loadings.res <- loadings.orig %*% diag(sqrt(eig[1:2]))
matplot(x=seq_along(stolpci), y=loadings.res, type="o", pch=16,
        xlab="", ylab="Korelacije", ylim=c(-1, 1), las=1,
        xaxt = "n")
legend("bottomleft", legend=c("GK1", "GK2"), col=1:2, pch=16)
abline(h=0, v=10.5)
axis(side = 1, at = 1:20, labels = stolpci, rotation='vertical', las=2)
```



Obe glavni komponenti sta v levi polovici pozitivno korelirani, ampak glavna komponenta 2 je v območju, ki nam povzroča probleme, saj korelacija ni zanemerljiva, ampak ni tudi tako visoka, da bi jo z zagotovostjo povezali z lastnostmi energije. Na desni polovici pa se glavni komponenti lepo ločita na pozitivno in negativno koreliranost.

# Faktorska analiza

## Slide 1

Želimo poenostaviti kompleksnost povezav med spremenljivkami energije in čustvene stabilnosti tako, da sva našla novo dva faktorja, ki predstavlja skupno opazovanim spremenljivkam. Na grafu vidimo, da pa se tokrat spremenljivke, ki so v povezavi z energijo in spremenljivke čustvene stabilnosti lepo ločujejo glede na faktorja.

```{r}
# standardizirani podatki
dfz <- scale(x=podatki)

# metoda ML, posevna rotacija oblimin
fa.obli <- fa(r=dfz, nfactors=2, rotate="oblimin", scores="none", covar=T, fm="ml")


# metoda ML, pravokotna rotacija varimax, faktorske vrednosti regresija
# TLI (Tucker Lewis Index) > 0.95
# RMSEA  < 0.05
fa.vari <- fa(r=dfz, nfactors=2, rotate="varimax", scores="regression",
              covar=F, fm="ml")

CM.vari <- fa.vari$loadings %*% t(fa.vari$loadings) 
razlika <- Rlik - CM.vari

matplot(x=seq_along(stolpci), y=fa.vari$loadings[, ], type="o", pch=16,
        main="Pattern", xlab="", ylab="utezi", ylim=c(-1, 1),
        las=1, xaxt="n")
legend("bottomleft", legend=c("FA1", "FA2"), col=1:2, pch=16)
abline(h=0, v=10.5)
axis(side = 1, at = 1:20, labels = stolpci, rotation='vertical', las=2)
```

Jasno je videti, da za te podatke in spremenljivke faktorska analiza ločuje bolje.

# Diskriminantna analiza

```{r}
# branje podatkov
data <- read.csv("dn5_data_wine.csv", header=T)
dfz <- scale(x=data[,2:ncol(data)])
skupine = data[,1]
spremenljivke = c("Alcohol", "Malic_acid", "Ash",
                  "Alcalinity_of_ash", "Magnesium", "Total_phenols",
                  "Flavanoids", "Nonflavanoid_phenols",
                  "Proanthocyanins", "Color_intensity", "Hue",
                  "OD280_OD315",  "Proline")
```

## Slide 1

Podatkovni okvir z nalovom Wine - vsebuje 13 različnih lastnosti vina iz kemijske analize treh različnih sort. Iz teh komponent želimo določiti kateri sorti pripada vino.

Za skupine imamo sorte vina, označene so z številkami $1$, $2$ in $3$ in posameznih vin je glede na sorto:

```{r, results=T}
df_vel_skupin = data.frame("skupina" = c(1:3),
                           "koliko" = c(59,71,48),
                           "delež" = round(c(59,71,48)/178*100,2))

kable(df_vel_skupin, digits = 2,
      caption="Velikost skupin.", col.names = c("sorta vina", "število vin", "delež %")) %>% 
  kable_styling(full_width=F, latex_options="hold_position")
```

Poglejmo si še kako se sorte vina razlikujejo v povprečjih glede na lastnost na grafu, s katerega lahko vidimo, da večina spremenljivk dobro loči med sortami vina, je pa nekaj takih kjer to ne velja.

```{r fig.dim=c(6,4), fig.cap="Povprečja neodvisnih spremenljivk po skupinah (standardizirane vrednosti).", fig.pos="H"}
# povprecja neodvisnih spremenljivk 
par(mar = c(3, 3, 3, 8), xpd = TRUE) 
agr <- aggregate(x=dfz, by=list(skupine), FUN=mean)
matplot(x=seq_along(spremenljivke), y=t(agr[, -1]), type="o", pch=16, col=1:3, xaxt="n", xlab="", ylab="Povprecje", las=1)
axis(side=1, at=seq_along(spremenljivke), labels=spremenljivke, las=2, cex.axis=0.5)
legend("topright", inset = c(-0.15, 0), legend=agr[, 1], col=1:3, pch=16)
```

## Slide 2

Izvedli smo linearno diskriminantno analizo na standardiziranih podatkih in zanimalo nas je med katerimi sortami vina bolje ločuje posamezna diskriminantna funkcija. 

Prva linearna diskriminantna funkcija je najbolje ločuvala med sorto vina 1 in sorto vina 3. Druga linearna diskriminantna funkcija pa najbolje ločuje med sorto vina 1 in 2 ter 3 in 2, kar se zelo dobro vidi na spodnjem grafu. 

```{r, results=T, fig.cap="Grafični prikaz porazdelitev vrednosti po skupinah.", fig.width=8, fig.height=4}
X <- dfz
sk <- as.factor(data[, "Class"])
lda.fit <- lda(x=X, grouping=sk, method="moment")

lda.predict <- predict(object=lda.fit, dimen=2)
xLD1 <- lda.predict$x[, "LD1"]
xLD2 <- lda.predict$x[, "LD2"]
oldpar <- par(las=1, mfrow=c(1, 2))
plot(density(xLD1[which(sk=="1")]), xlim=c(-7, 8), ylim=c(0, 0.6), main="LD1")
lines(density(xLD1[which(sk=="2")]), col=2)
lines(density(xLD1[which(sk=="3")]), col=3)
legend("topleft", legend=levels(sk), col=1:3, pch=16)
plot(density(xLD2[which(sk=="1")]), xlim=c(-8, 5), ylim=c(0, 0.5), main="LD2")
lines(density(xLD2[which(sk=="2")]), col=2)
lines(density(xLD2[which(sk=="3")]), col=3)
legend("topleft", legend=levels(sk), col=1:3, pch=16)
par(oldpar)
```

Pri prvi diskriminantni funkciji je imela intenziteta barve pomembno vlogo, pri drugi pa na primer ne. 

## Slide 3

```{r, fig.cap="Prikaz enot glede na pravo pripadnost sorti vina(levo) in pripadnost posamezni skupini glede na klasifikacijo po LDA(desno).", fig.width=9}
# pripadnost sorta vina 1-3
par(mfrow=c(1,2))
plot(x=xLD1, y=xLD2, col=sk, xlim=c(-6, 6), ylim=c(-5, 5), main="Pripadnost (prava)", asp=1)
legend("bottomleft", legend=levels(sk), col=1:3, pch=16)
# pripadnost LDA
# barva oznacuje pripadnost skupini po klasifikaciji LDA
plot(x=xLD1, y=xLD2, col=lda.predict$class, xlim=c(-6, 6), ylim=c(-5, 5), main="Pripadnost (klasifikacija)", asp=1)
legend("bottomleft", legend=levels(lda.predict$class), col=1:3, pch=16)
```



