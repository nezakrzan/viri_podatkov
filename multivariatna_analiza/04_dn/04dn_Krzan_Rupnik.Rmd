---
title: "Domača naloga 4"
author: "Neža Kržan, Tom Rupnik"
output:
  pdf_document:
    fig_caption: true
    number_sections: true
  # word_document: default
header-includes:
- \usepackage[slovene]{babel}
- \usepackage{float}
- \usepackage[T1]{fontenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.pos = "H",
	message = FALSE,
	warning = FALSE,
	results = F, fig.height = 4, fig.width = 5
)

library(kableExtra)
library(MASS)
library(mclust)

set.seed(2024)
```

```{r}
# branje podatkov
data <- read.csv("dn5_data_wine.csv", header=T)
```

# Opis podatkov

Za analizo sva si izbrala podatkovni okvir z naslovom _Wine_, ki vsebuje `r nrow(data)` vrstic. Enoto oz. vrstico podatkov predstavlja posamezno vino. Podatki vsebujejo vrednosti kemijske analize treh različnih sort vina iz enake regije v Italiji.  Analiza vsebuje vrednosti 13 različnih komponent (lastnosti) najdenih v vsaki izmed treh sort vina. Iz teh komponent želimo določiti kateri sorti pripada vino (stolpec _Class_). Komponente oz. stolpci v podatkovne okvirju so:

```{r, results=T}
df_stolpci = data.frame("Ime" = c("Class", "Alcohol", "Malic_acid", "Ash",
                                  "Alcalinity_of_ash", "Magnesium", "Total_phenols",
                                  "Flavanoids", "Nonflavanoid_phenols",
                                  "Proanthocyanins", "Color_intensity", "Hue",
                                  "OD280_OD315",  "Proline"),
                        "Pomen" = c("razred (uporabimo za analizo)", "% alkohola",
                                    "jabolčna kislina", "vsebnost pepela",
                                    "alkalnost pepela", "Magnezij", 
                                    "skupaj fenolov",  "rastlinske kemikalije(fenoli)",
                                    "fenolne spojine(za okus, aromo, barvo)",
                                    "kondenzirani tanini", "intenziteta barve",
                                    "barvni odtenek vina", "razmerje ocene vsebnosti fenolnih spojin", 
                                    "aminokislina"))


kable(df_stolpci, 
      caption="Spremenljivke v podatkovnem okvirju.") %>% 
  kable_styling(full_width=F, latex_options="hold_position", font_size = 9)
```

Poglejmo si osnovne opisne statistike spremenljivk.

```{r, results=T}
df_stat = data.frame("Ime" = c( "Alcohol", "Malic_acid", "Ash",
                                  "Alcalinity_of_ash", "Magnesium", "Total_phenols",
                                  "Flavanoids", "Nonflavanoid_phenols",
                                  "Proanthocyanins", "Color_intensity", "Hue",
                                  "OD280_OD315",  "Proline"),
                     "min" = c(min(data$Alcohol),
                               min(data$Malic_acid), min(data$Ash),
                               min(data$Alcalinity_of_ash), min(data$Magnesium),
                               min(data$Total_phenols), min(data$Flavanoids),
                               min(data$Nonflavanoid_phenols), 
                               min(data$Proanthocyanins), 
                               min(data$Color_intensity), min(data$Hue),
                               min(data$OD280_OD315),  min(data$Proline)),
                     "max" = c(max(data$Alcohol),
                               max(data$Malic_acid), max(data$Ash),
                               max(data$Alcalinity_of_ash), max(data$Magnesium),
                               max(data$Total_phenols), max(data$Flavanoids),
                               max(data$Nonflavanoid_phenols), 
                               max(data$Proanthocyanins), 
                               max(data$Color_intensity), max(data$Hue),
                               max(data$OD280_OD315),  max(data$Proline)),
                     "povprecje" = c(mean(data$Alcohol),
                               mean(data$Malic_acid), mean(data$Ash),
                               mean(data$Alcalinity_of_ash), mean(data$Magnesium),
                               mean(data$Total_phenols), mean(data$Flavanoids),
                               mean(data$Nonflavanoid_phenols), 
                               mean(data$Proanthocyanins), 
                               mean(data$Color_intensity), mean(data$Hue),
                               mean(data$OD280_OD315),  mean(data$Proline)))

kable(df_stat, digits = 2,
      caption="Opisna statistika spremenljivk.") %>% 
  kable_styling(full_width=F, latex_options="hold_position", font_size = 9)
```

Za boljšo predstavo o spremenljivkah, si poglejmo še njihove histograme.

```{r echo=FALSE, fig.cap="Porazdelitev spremenljivk v podatkovnem okviru vin.", fig.dim=c(8,8), message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
g1 = ggplot(data, aes(x=Alcohol))+geom_histogram()+xlab("")+ylab("frekvenca")+ggtitle("Alcohol")+theme_bw()
g2 = ggplot(data, aes(x=Malic_acid))+geom_histogram( )+xlab("")+ylab("frekvenca")+ggtitle("Malic acid")+theme_bw()
g3 = ggplot(data, aes(x=Ash))+geom_histogram( )+xlab("")+ylab("frekvenca")+ggtitle("Ash")+theme_bw()
g4 = ggplot(data, aes(x=Alcalinity_of_ash))+geom_histogram()+xlab("")+ylab("frekvenca")+ggtitle("Alcalinity of ash")+theme_bw()
g5 = ggplot(data, aes(x=Magnesium))+geom_histogram(binwidth=1)+xlab("")+ylab("frekvenca")+ggtitle("Magnesium")+theme_bw()
g6 = ggplot(data, aes(x=Total_phenols))+geom_histogram( )+xlab("")+ylab("frekvenca")+ggtitle("Total phenols")+theme_bw()
g7 = ggplot(data, aes(x=Flavanoids))+geom_histogram( )+xlab("")+ylab("frekvenca")+ggtitle("Flavanoids")+theme_bw()
g8 = ggplot(data, aes(x=Nonflavanoid_phenols))+geom_histogram()+xlab("")+ylab("frekvenca")+ggtitle("Nonflavanoid phenols")+theme_bw()
g9 = ggplot(data, aes(x=Proanthocyanins))+geom_histogram()+xlab("")+ylab("frekvenca")+ggtitle("Proanthocyanins")+theme_bw()
g10 = ggplot(data, aes(x=Color_intensity))+geom_histogram( )+xlab("")+ylab("frekvenca")+ggtitle("Color intensity")+theme_bw()
g11 = ggplot(data, aes(x=Hue))+geom_histogram( )+xlab("")+ylab("frekvenca")+ggtitle("Hue")+theme_bw()
g12 = ggplot(data, aes(x=OD280_OD315))+geom_histogram( )+xlab("")+ylab("frekvenca")+ggtitle("OD280/OD315")+theme_bw()
g13 = ggplot(data, aes(x=Proline))+geom_histogram( )+xlab("")+ylab("frekvenca")+ggtitle("Proline")+theme_bw()

grid.arrange(g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,g13,
             ncol = 4)
```


# Skupine in povezanost

Za skupine torej imamo sorte vina, označene so z številkami $1$, $2$ in $3$.

```{r}
# standardizirani podatki
dfz <- scale(x=data[,2:ncol(data)])

skupine = data[,1]
spremenljivke = c("Alcohol", "Malic_acid", "Ash",
                  "Alcalinity_of_ash", "Magnesium", "Total_phenols",
                  "Flavanoids", "Nonflavanoid_phenols",
                  "Proanthocyanins", "Color_intensity", "Hue",
                  "OD280_OD315",  "Proline")
```

```{r results='hide'}
# velikost skupin
tabela <- cbind(table(skupine), 100 * proportions(table(skupine)))
colnames(tabela) <- c("f", "f%")
tabela
```

```{r, results=T}
df_vel_skupin = data.frame("skupina" = c(1:3),
                           "koliko" = c(59,71,48),
                           "delež" = round(c(59,71,48)/178*100,2))

kable(df_vel_skupin, digits = 2,
      caption="Velikost skupin.") %>% 
  kable_styling(full_width=F, latex_options="hold_position")
```

Iz zgornje tabele vidimo, da je največ enot prisotnih v skupini **2** (71) in najmanj v skupini **3** (48). Preostanek enot je v skupini **1** in sicer 59.

```{r fig.dim=c(6,4), fig.cap="Povprečja neodvisnih spremenljivk po skupinah (standardizirane vrednosti).", fig.pos="H"}
# povprecja neodvisnih spremenljivk 
par(mar = c(3, 3, 3, 8), xpd = TRUE) 
agr <- aggregate(x=dfz, by=list(skupine), FUN=mean)
matplot(x=seq_along(spremenljivke), y=t(agr[, -1]), type="o", pch=16, col=1:3, xaxt="n", xlab="", ylab="Povprecje", las=1)
axis(side=1, at=seq_along(spremenljivke), labels=spremenljivke, las=2, cex.axis=0.5)
legend("topright", inset = c(-0.15, 0), legend=agr[, 1], col=1:3, pch=16)
```


Iz grafičnega prikaza lahko vidimo, da večina spremenljivk dobro loči med skupinami, je pa nekaj takih kjer to ne velja. Primer take spremenljivke je na primer _Hue_ ali _Ash_. z grafa tudi vidimo, da posamezna spremenljivka ima različna povprečja v posamezni skupini(sorti vina). Če so razlike v povprečjih majhne, lahko pričakujemo, da bomo z diskriminatnimi funkcijami slabo ločevali med skupinami.

# Predpostavke

Pri diskriminantni analizi imamo nekaj predpostavk, katerim morajo podatki zadoščati. V najinem podatkovnem okviru imava vsaj dve skupini - imava 3 skupine(3 sorte vina) **1**, **2**, **3**, in zgoraj smo videli v tabeli 3, da ima vsaka skupina več kot dve enoti. Imava tudi manjše število spremenljivk kot enot(zapisano je v začetku naloge, koliko enot in koliko spremenljivk imamo v podatkovnem okviru). Ker pa skupine niso enako velike, ne bova predpostavila enako velikih skupin.

Sedaj moramo preveriti predpostavko o odsotnosti multikolinearnosti, torej, da nobena od spremenljivk ni popolna linearna kombinacija ostalih. Težavo imamo, ko je katera od korelacij med spremenljivkama zelo blizu 1(npr. nad 0.95). V ta namen si najprej poglejmo korelacijsko matriko.

```{r results='hide'}
# multikolinearnost
cor(dfz)
sort(cor(dfz)[cor(dfz)<1])

# rang matrike X'X
X <- as.matrix(dfz)
dim(X)
qr(X)$rank
qr(t(X) %*% X)$rank

# lastne vrednosti matrike X'X
eigen(t(X) %*% X, symmetric=T, only.values=T)

# obcutljivost ali pogojenostno stevilo matrike X'X
kappa(t(X) %*% X)
```


```{r, fig.cap="Korelacijska matrika spremenljivk v podatkovnem okviru.", fig.height=5, fig.width=8}
library(corrplot)
colors10 <- colorRampPalette(c("#0000aa","white","#aa0000"))(10)
corrplot.mixed(cor(data,
                   method = "spearman"),
               lower.col = colors10, upper.col = colors10,
               tl.col="black",   tl.cex = 0.5, number.cex = 0.8)
```

Iz matrike vidimo, da nimamo večjih korelacij, kar bi lahko pomenilo, da multikolinearnost ni prisotna(sicer se le ta zelo pozno vidi, npr. šele, ko imamo koeficiente nad 0.95). Če sestavimo matriko $XX'$, kjer imamo v matriki $X$ podatke naših spremenljivk, je njen rang enak `r qr(t(X) %*% X)$rank` in njena občutljivost oz. pogojenostno število je enako `r round(kappa(t(X) %*% X), 3)`, kar je dokaj majhno, torej res lahko rečemo, da je predpostavka o odsotnosti multikolinearnosti izpolnjena.

Naslednja predpostavka pa je homogoenost, ki zahteva, da so vse kovariančne matrike skupin na populaciji enake. Pri prevrjanju te predpostavke si pomagamo z Box M-testom, kjer preverjamo $H_0:$ _kovariančne matrike po skupinah(po sortah vin) so enake_.

```{r results=T}
# homogenost
# kovariancne matrike na pogled
# by(data=dfz, INDICES=skupine, FUN=var)
# Box M-test
# H0: kovariancne matrike po skupinah so enake
source(file="BoxMTest.R")
BoxMTest(X=dfz, cl=as.factor(skupine), alpha=0.05, test=F)
```

Iz zgornjega zapisa vidimo, da je $p < 0.05$, torej zavračamo ničelno hipotezo, kar pomeni, da pogoj ni izpolnjen, ampak nadaljujemo z diskriminantno analizo.

# Analiza

Izvedimo sedaj linearno diskriminantno analizo na standardiziranih podatkih. 

```{r, results=T}
X <- dfz
sk <- as.factor(data[, "Class"])
lda.fit <- lda(x=X, grouping=sk, method="moment")
```

S prvo diskriminantno funkcijo pojasnimo 68% razlik med povprečji spremenljivk, z drugo kanonično korelacijo pa 31% razlik med povprečji spremenljivk.

Poglejmo si še ali sta diskriminantni funkciji statistično značilni, kjer preverjamo domnevo $H_0:$ _povprečja diskriminantnih funkcij po skupinah(po sortah vin) enaka_.

```{r, results=T}
n <- nrow(X)
p <- ncol(X)
q <- length(levels(sk)) - 1
ysk <- model.matrix(~ sk)[, -1, drop=F]
cca <- cancor(x=X, y=ysk)
source(file="sklop51_lda_testCC.R")

kable(round(testCC(cor=cca$cor, n=n, p=p, q=q)$sigTest, 5), 
      caption="Statistična značilnost diskriminantnih funckij.") %>% 
  kable_styling(full_width=F, latex_options="hold_position")
```


Pri stopnji značilnosti 0.05 lahko na podlagi $p-vrednost$ za obe diskriminantni funkciji zavrnemo domnevo $H_0$ in sprejememo, da so povprečja po sortah vin za obe diskriminantni funkciji različna. 

Sedaj nas seveda zanima med katerimi skupinami bolje ločuje posamezna diskriminantna funkcija. 

```{r, results=T}
lda.fit$scaling

prior <- lda.fit$prior
means <- colSums(prior * lda.fit$means)
dm <- scale(lda.fit$means, center=means, scale=F) %*% lda.fit$scaling

kable(data.frame(class = c(1,2,3), round(dm, digits=3)), caption="Povprečja po skupinah",
      col.names = c("skupina", "LD1", "LD2")) %>% 
  kable_styling(full_width=F, latex_options="hold_position")
```

Prva linearna diskriminantna funkcija najbolj ločuje med sorto vina **1** in sorto vina **3**. Druga linearna diskriminantna funkcija pa najbolje ločuje med sorto vina **1** in **2**.

Poglejmo si spremenljivke, ki imajo največji vpliv na vrednosti diskriminantne funkcije, ker so z posamezno funkcijo močno korelirane. Torej spremenjivke, ki najbolj ločujejo med skupinami. 

V največji meri se nam splača govoriti o intenziteti barve( _Color_intensity_), razmerju ocene vsebnosti fenolnih spojin( _OD280_OD315_), aminokislinah( _Proline_) pri prvi diskriminantni funkciji. 
Sklepamo lahko, da je pri sorti vina **1**  pomembna predvsem razmerje OD280/OD315, aminokisline( _Proline_), pri sorti **3** pa intenziteta barve( _Color_intensity_).

Prav tako se splača govoriti o aminokislinah( _Proline_), % alkohola( _Alcohol_), vsebovanost pepela( _Ash_) pri drugi diskriminantni funkciji. Šibko koreliranost z drugo diskriminantno funkcijo pa imajo tudi intenziteta barve( _Color_intensity_) in spremenljivka _Flavanoids_.

Pri sorti vina **1** je pomemben % alkohola( _Alcohol_), aminokisline( _Proline_) in vsebovanost pepela(_Ash_), pri sorti **2** pa _Flavanoids_ in alkalnost pepela(*Alcalinity_of_ash*).

Poglejmo si še grafično kako je z ločevanjem po skupinah. Na spodnjem grafu vidimo, da se pri LD1 porazdelitve skupin ne prekrivajo tako zelo, s tem ko pri LD2, ne ločujemo med skupinama **1** in **3**, sorta vina **1** pa ima težičše precej v levo. Vidimo, da ne glede na to kaj smo opisali, da dejanska sposobnost ločevanja po sortah pri drugi funkciji zelo mahna, pri prvi pa bi to bilo možno.

```{r, results=T, fig.cap="Grafični prikaz porazdelitev vrednosti po skupinah.", fig.width=8, fig.height=4}
lda.predict <- predict(object=lda.fit, dimen=2)
xLD1 <- lda.predict$x[, "LD1"]
xLD2 <- lda.predict$x[, "LD2"]
oldpar <- par(las=1, mfrow=c(1, 2))
plot(density(xLD1[which(sk=="1")]), xlim=c(-7, 8), ylim=c(0, 0.6), main="LD1")
lines(density(xLD1[which(sk=="2")]), col=2)
lines(density(xLD1[which(sk=="3")]), col=3)
legend("topleft", legend=levels(sk), col=1:3, pch=16)
plot(density(xLD2[which(sk=="1")]), xlim=c(-8, 5), ylim=c(0, 0.5), main="LD2")
lines(density(xLD2[which(sk=="2")]), col=2)
lines(density(xLD2[which(sk=="3")]), col=3)
legend("topleft", legend=levels(sk), col=1:3, pch=16)
par(oldpar)
```

# Kvaliteta ocenjenega modela

Če si ponovno ogledamo tabelo iz sklopa 2.

```{r results=T}
kable(df_vel_skupin, digits = 2,
      caption="Velikost skupin.") %>% 
  kable_styling(full_width=F, latex_options="hold_position")
```

V našem primeru imamo največ enot v skupini **2** (71). V primeru, da bi vse enote razvrstili v največjo skupino, bi bil delež pravilno razvrščenih enot enak `r round(71/178*100,2)`%. 

```{r}
# klasifikacijska tabela in delez pravilno razvrscenih enot
# vrstica = original, stolpec = LDA model
tabela <- table(sk, lda.predict$class) # prave vrednosti na diagonali
sum(diag(tabela)) / sum(tabela)
```

```{r results=T}
df = data.frame("class"= c(1:3),
                "1" = c(59,0,0),
                "2" = c(0, 71, 0),
                "3" = c(0, 0, 48))

kable(df, caption="Klasifikacijska tabela.",
      col.names = c("", "1", "2", "3")) %>% 
  column_spec(1,bold=T) %>%
  kable_styling(full_width=F, latex_options="hold_position")
```

Že iz tabele je razvidno, da vse enote razvrstimo pravilno (izven diagonale so vrednosti 0), torej je delež pravilno razvrščenih enak 1. Prav tako je popravljen Randlov indeks enak 1.

```{r}
# adjusted rand index
adjustedRandIndex(x=sk, y=lda.predict$class)
```

Preverimo še kakšne vrednosti bi dobili v primeru uporabe navzkrižnega preverjanja(CV), ker smo se prej učili na istih podatkih kot smo potem model tudi testirali, zato je možnost, da precenjujemo. Želimo pa si bolj verodostojne ocene oziroma nepristranske ocene za delež pravilno razvrščenih enot.

```{r}
# nepristranska ocena za delez pravilno razvrscenih enot
lda.fit <- lda(x=X, grouping=sk, method="moment", CV=T)
tabelaCV <- table(sk, lda.fit$class)
sum(diag(tabelaCV)) / sum(tabelaCV)
adjustedRandIndex(x=sk, y=lda.fit$class)
```

```{r results=T}
df_CV = data.frame("class"= c(1:3),
                "1" = c(59,1,0),
                "2" = c(0, 69, 0),
                "3" = c(0, 1, 48))

kable(df_CV, caption="Klasifikacijska tabela (CV).",
      col.names = c("", "1", "2", "3")) %>% 
  column_spec(1,bold=T) %>%
  kable_styling(full_width=F, latex_options="hold_position")
```

Iz tabele je vidno, da tokrat dobimo nekoliko slabše rezultate. V primerjavi s prejšnjo razvrtitvijo, smo tokrat dve enoti iz skupine **2** napačno razvrstili. Eno enoto smo uvrstili v skupino **1** in eno v skupino **3**. Nepristranska ocena za delež pravilno razvrščenih enot je enaka `r round(sum(diag(tabelaCV)) / sum(tabelaCV),3)`, popravljen Randov indeks pa `r round(adjustedRandIndex(x=sk, y=lda.fit$class),3)`.

# Grafični prikaz

Naprej si oglejmo prikaz enot glede na pravo pripadnost sorti vina, nato pa še  pripadnost posamezni skupini glede na klasifikacijo po LDA.

```{r, fig.cap="Prikaz enot glede na pravo pripadnost sorti vina(levo) in pripadnost posamezni skupini glede na klasifikacijo po LDA(desno).", fig.width=9}
# pripadnost sorta vina 1-3
par(mfrow=c(1,2))
plot(x=xLD1, y=xLD2, col=sk, xlim=c(-6, 6), ylim=c(-5, 5), main="Pripadnost (prava)", asp=1)
legend("bottomleft", legend=levels(sk), col=1:3, pch=16)
# pripadnost LDA
# barva oznacuje pripadnost skupini po klasifikaciji LDA
plot(x=xLD1, y=xLD2, col=lda.predict$class, xlim=c(-6, 6), ylim=c(-5, 5), main="Pripadnost (klasifikacija)", asp=1)
legend("bottomleft", legend=levels(lda.predict$class), col=1:3, pch=16)
```

Že iz levega grafa se vidi, da so sorte vina med seboj dobro ločene oz. se med seboj ne prekrivajo. Zato ni presenečenje, da je taka situacija tudi v primeru klasifikacije po LDA. Skupine se lepo ločujejo glede na prvo diskriminantno funkcijo(gledamo vodoravno) in glede na drugo(gledamo navpično).

Grafa sta med seboj identična zato lahko zaključimo, da diskriminantna analiza odlično ločuje med skupinami.

